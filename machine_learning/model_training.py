# machine_learning/model_training.py
"""
Training m√¥ h√¨nh XGBoost Multi-Output ƒë·ªÉ d·ª± ƒëo√°n th·ªùi ti·∫øt
S·ª≠ d·ª•ng d·ªØ li·ªáu t·ª´ database PostgreSQL v·ªõi schema m·ªõi
"""
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="pandas")
warnings.filterwarnings("ignore", message=".*supautils.*")  # ch·∫∑n supautils lu√¥n
warnings.filterwarnings("ignore")  # ch·∫∑n t·∫•t c·∫£ n·∫øu c·∫ßn

import pandas as pd
import joblib
import os
import sys
from datetime import datetime
from xgboost import XGBRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np


sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from data_pipeline.data_storage import connect_to_db

MODEL_DIR = os.path.join(os.path.dirname(__file__), 'models')
MODEL_PATH = os.path.join(MODEL_DIR, 'weather_xgboost_multi.pkl')
os.makedirs(MODEL_DIR, exist_ok=True)

def load_data_for_training(conn, province_id=None, limit=500000):
    """
    Load d·ªØ li·ªáu t·ª´ database ƒë·ªÉ training
    
    Args:
        province_id: N·∫øu None th√¨ l·∫•y t·∫•t c·∫£ t·ªânh, n·∫øu c√≥ gi√° tr·ªã th√¨ ch·ªâ l·∫•y t·ªânh ƒë√≥
        limit: S·ªë l∆∞·ª£ng b·∫£n ghi t·ªëi ƒëa
    
    Returns:
        DataFrame v·ªõi c√°c c·ªôt t·ª´ b·∫£ng weather_data
    """
    # conn = connect_to_db()
    
    # Query d·ª±a tr√™n schema m·ªõi
    base_query = """
        SELECT 
            timestamp,
            province_id,
            -- Nhi·ªát ƒë·ªô & ƒë·ªô ·∫©m
            temperature_2m,
            apparent_temperature,
            relative_humidity_2m,
            -- L∆∞·ª£ng m∆∞a & m√¢y
            precipitation,
            rain,
            showers,
            cloud_cover,
            cloud_cover_low,
            cloud_cover_mid,
            cloud_cover_high,
            weather_code,
            -- Gi√≥ & √°p su·∫•t
            wind_speed_10m,
            wind_direction_10m,
            wind_gusts_10m,
            pressure_msl,
            -- B·ª©c x·∫° & n·∫Øng
            shortwave_radiation,
            direct_radiation,
            uv_index,
            sunshine_duration
        FROM weather_data 
        WHERE temperature_2m IS NOT NULL 
          AND pressure_msl IS NOT NULL
    """
    
    if province_id:
        query = base_query + " AND province_id = %s ORDER BY timestamp DESC LIMIT %s"
        df = pd.read_sql(query, conn, params=(province_id, limit))
    else:
        query = base_query + " ORDER BY timestamp DESC LIMIT %s"
        df = pd.read_sql(query, conn, params=(limit,))
    
    conn.close()
    print(f"‚úÖ ƒê√£ t·∫£i {len(df)} b·∫£n ghi t·ª´ database")
    
    # Ki·ªÉm tra d·ªØ li·ªáu
    if len(df) == 0:
        print("‚ö†Ô∏è  Kh√¥ng c√≥ d·ªØ li·ªáu trong database!")
        return None
    
    # Hi·ªÉn th·ªã th√¥ng tin v·ªÅ d·ªØ li·ªáu
    print(f"\nüìä Th√¥ng tin d·ªØ li·ªáu:")
    print(f"   ‚Ä¢ Kho·∫£ng th·ªùi gian: {df['timestamp'].min()} ƒë·∫øn {df['timestamp'].max()}")
    print(f"   ‚Ä¢ S·ªë t·ªânh: {df['province_id'].nunique()}")
    print(f"   ‚Ä¢ C√°c c·ªôt c√≥ s·∫µn: {', '.join(df.columns.tolist())}")
    
    return df

def feature_engineering(df):
    """
    T·∫°o features cho model
    Bao g·ªìm: lag features, rolling features, time features
    
    Args:
        df: DataFrame t·ª´ database
    
    Returns:
        X: Features
        y: Targets
        feature_cols: Danh s√°ch t√™n c√°c features
    """
    print("\nüîß ƒêang t·∫°o features...")
    
    df = df.sort_values(['province_id', 'timestamp'])
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # Fill missing values v·ªõi gi√° tr·ªã h·ª£p l√Ω
    fill_values = {
        'apparent_temperature': df['temperature_2m'],  # N·∫øu thi·∫øu th√¨ d√πng temp th∆∞·ªùng
        'precipitation': 0,
        'rain': 0,
        'showers': 0,
        'cloud_cover': 50,
        'cloud_cover_low': 0,
        'cloud_cover_mid': 0,
        'cloud_cover_high': 0,
        'wind_gusts_10m': df['wind_speed_10m'],  # N·∫øu thi·∫øu gust th√¨ d√πng wind speed
        'shortwave_radiation': 0,
        'direct_radiation': 0,
        'uv_index': 0,
        'sunshine_duration': 0,
        'weather_code': 1
    }
    
    for col, default_val in fill_values.items():
        if col in df.columns:
            df[col].fillna(default_val, inplace=True)
    
    # =========================================================================
    # LAG FEATURES - D·ªØ li·ªáu t·ª´ c√°c gi·ªù tr∆∞·ªõc
    # =========================================================================
    print("   üìå T·∫°o lag features...")
    lags = [1, 2, 3, 6, 12, 24]  # 1h, 2h, 3h, 6h, 12h, 24h tr∆∞·ªõc
    
    lag_cols = [
        'temperature_2m',
        'relative_humidity_2m', 
        'wind_speed_10m',
        'pressure_msl',
        'precipitation',
        'cloud_cover'
    ]
    
    for lag in lags:
        for col in lag_cols:
            if col in df.columns:
                df[f'{col}_lag{lag}'] = df.groupby('province_id')[col].shift(lag)
    
    # =========================================================================
    # ROLLING FEATURES - Trung b√¨nh tr∆∞·ª£t
    # =========================================================================
    print("   üìå T·∫°o rolling features...")
    rolls = [3, 6, 24]  # 3h, 6h, 24h
    
    for w in rolls:
        # Temperature
        df[f'temp_roll_mean_{w}'] = df.groupby('province_id')['temperature_2m'].transform(
            lambda x: x.rolling(w, min_periods=1).mean()
        )
        df[f'temp_roll_std_{w}'] = df.groupby('province_id')['temperature_2m'].transform(
            lambda x: x.rolling(w, min_periods=1).std()
        )
        df[f'temp_roll_min_{w}'] = df.groupby('province_id')['temperature_2m'].transform(
            lambda x: x.rolling(w, min_periods=1).min()
        )
        df[f'temp_roll_max_{w}'] = df.groupby('province_id')['temperature_2m'].transform(
            lambda x: x.rolling(w, min_periods=1).max()
        )
        
        # Precipitation - T·ªïng l∆∞·ª£ng m∆∞a
        df[f'precip_roll_sum_{w}'] = df.groupby('province_id')['precipitation'].transform(
            lambda x: x.rolling(w, min_periods=1).sum()
        )
        
        # Humidity
        df[f'humidity_roll_mean_{w}'] = df.groupby('province_id')['relative_humidity_2m'].transform(
            lambda x: x.rolling(w, min_periods=1).mean()
        )
        
        # Pressure
        df[f'pressure_roll_mean_{w}'] = df.groupby('province_id')['pressure_msl'].transform(
            lambda x: x.rolling(w, min_periods=1).mean()
        )
        
        # Wind
        df[f'wind_roll_mean_{w}'] = df.groupby('province_id')['wind_speed_10m'].transform(
            lambda x: x.rolling(w, min_periods=1).mean()
        )
        df[f'wind_roll_max_{w}'] = df.groupby('province_id')['wind_speed_10m'].transform(
            lambda x: x.rolling(w, min_periods=1).max()
        )
    
    # =========================================================================
    # TIME FEATURES - ƒê·∫∑c tr∆∞ng th·ªùi gian
    # =========================================================================
    print("   üìå T·∫°o time features...")
    df['hour'] = df['timestamp'].dt.hour
    df['dayofweek'] = df['timestamp'].dt.dayofweek
    df['month'] = df['timestamp'].dt.month
    df['day'] = df['timestamp'].dt.day
    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
    
    # Season (m√πa)
    df['season'] = df['month'].map({
        12: 0, 1: 0, 2: 0,  # ƒê√¥ng
        3: 1, 4: 1, 5: 1,   # Xu√¢n
        6: 2, 7: 2, 8: 2,   # H√®
        9: 3, 10: 3, 11: 3  # Thu
    })
    
    # Time of day (bu·ªïi trong ng√†y)
    df['time_of_day'] = pd.cut(
        df['hour'],
        bins=[0, 6, 12, 18, 24],
        labels=[0, 1, 2, 3],
        include_lowest=True
    ).astype(int)   

    
    # =========================================================================
    # INTERACTION FEATURES - T∆∞∆°ng t√°c gi·ªØa c√°c bi·∫øn
    # =========================================================================
    print("   üìå T·∫°o interaction features...")
    
    # Nhi·ªát ƒë·ªô x ƒê·ªô ·∫©m (c·∫£m gi√°c oi b·ª©c)
    df['temp_humidity_interaction'] = df['temperature_2m'] * df['relative_humidity_2m'] / 100
    
    # Nhi·ªát ƒë·ªô x Gi√≥ (c·∫£m gi√°c l·∫°nh do gi√≥)
    df['temp_wind_interaction'] = df['temperature_2m'] * df['wind_speed_10m']
    
    # √Åp su·∫•t x ƒê·ªô ·∫©m (kh·∫£ nƒÉng m∆∞a)
    df['pressure_humidity_interaction'] = df['pressure_msl'] * df['relative_humidity_2m'] / 100
    
    # Cloud cover total (t·ªïng ƒë·ªô che ph·ªß m√¢y)
    if all(col in df.columns for col in ['cloud_cover_low', 'cloud_cover_mid', 'cloud_cover_high']):
        df['cloud_cover_total'] = df['cloud_cover_low'] + df['cloud_cover_mid'] + df['cloud_cover_high']
    
    # =========================================================================
    # TARGET VARIABLES - D·ª± ƒëo√°n 1 gi·ªù t·ªõi
    # =========================================================================
    print("   üìå T·∫°o target variables...")
    
    target_cols = [
        'temperature_2m',           # Nhi·ªát ƒë·ªô
        'relative_humidity_2m',     # ƒê·ªô ·∫©m
        'precipitation',            # L∆∞·ª£ng m∆∞a
        'wind_speed_10m',           # T·ªëc ƒë·ªô gi√≥
        'pressure_msl',             # √Åp su·∫•t
        'cloud_cover'               # ƒê·ªô ph·ªß m√¢y (thay v√¨ visibility)
    ]
    
    # T·∫°o target cho 1 gi·ªù t·ªõi
    for col in target_cols:
        if col in df.columns:
            df[f'{col}_next'] = df.groupby('province_id')[col].shift(-1)
    
    # =========================================================================
    # DROP ROWS WITH MISSING VALUES
    # =========================================================================
    print("   üìå Lo·∫°i b·ªè d·ªØ li·ªáu thi·∫øu...")
    initial_rows = len(df)
    df.dropna(inplace=True)
    final_rows = len(df)
    print(f"   ‚úÖ ƒê√£ lo·∫°i b·ªè {initial_rows - final_rows} d√≤ng c√≥ gi√° tr·ªã thi·∫øu")
    
    # =========================================================================
    # SELECT FEATURES AND TARGETS
    # =========================================================================
    # C√°c c·ªôt kh√¥ng d√πng l√†m feature
    exclude_cols = [
        'timestamp', 'province_id', 'weather_code', 
        'apparent_temperature',  # ƒê√£ c√≥ temp_humidity_interaction
        'rain', 'showers',  # ƒê√£ c√≥ precipitation
        'wind_direction_10m',  # Direction kh√¥ng quan tr·ªçng b·∫±ng speed
        'wind_gusts_10m',  # ƒê√£ c√≥ wind_roll_max
        'shortwave_radiation', 'direct_radiation',  # T∆∞∆°ng quan cao v·ªõi sunshine_duration
        'sunshine_duration',  # C√≥ th·ªÉ b·ªè n·∫øu kh√¥ng c·∫ßn thi·∫øt
        'uv_index',  # UV c√≥ th·ªÉ t√≠nh t·ª´ hour v√† month
    ] + target_cols + [f'{c}_next' for c in target_cols]
    
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    target_cols_next = [f'{c}_next' for c in target_cols]
    
    X = df[feature_cols]
    y = df[target_cols_next]
    
    print(f"\n‚úÖ Feature engineering ho√†n t·∫•t!")
    print(f"   ‚Ä¢ S·ªë features: {len(feature_cols)}")
    print(f"   ‚Ä¢ S·ªë samples: {len(X)}")
    print(f"   ‚Ä¢ Target variables: {', '.join(target_cols)}")
    
    return X, y, feature_cols

def train(province_id=None, save_path=MODEL_PATH):
    """
    Hu·∫•n luy·ªán m√¥ h√¨nh XGBoost Multi-Output
    
    Args:
        province_id: N·∫øu None th√¨ train cho t·∫•t c·∫£ t·ªânh, n·∫øu c√≥ gi√° tr·ªã th√¨ ch·ªâ train cho t·ªânh ƒë√≥
        save_path: ƒê∆∞·ªùng d·∫´n l∆∞u model
    
    Returns:
        bool: True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    print("\n" + "="*80)
    print("üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN M√î H√åNH XGBOOST MULTI-OUTPUT")
    print("="*80)
    
    # =========================================================================
    # 1. LOAD DATA
    # =========================================================================
    print("\n[B∆Ø·ªöC 1/5] üì• ƒêang t·∫£i d·ªØ li·ªáu t·ª´ database...")
    conn = connect_to_db() 
    df = load_data_for_training(conn, province_id=province_id, limit=500000)
    conn.close()
    if df is None or len(df) < 1000:
        print("\n‚ùå TH·∫§T B·∫†I: Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán (c·∫ßn √≠t nh·∫•t 1000 b·∫£n ghi)")
        print("üí° Vui l√≤ng ch·∫°y data collection ƒë·ªÉ thu th·∫≠p d·ªØ li·ªáu:")
        print("   python data_pipeline/data_collection.py")
        return False
    
    # =========================================================================
    # 2. FEATURE ENGINEERING
    # =========================================================================
    print("\n[B∆Ø·ªöC 2/5] üîß ƒêang x·ª≠ l√Ω v√† t·∫°o features...")
    try:
        X, y, feature_cols = feature_engineering(df)
    except Exception as e:
        print(f"\n‚ùå TH·∫§T B·∫†I: L·ªói khi t·∫°o features: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    if len(X) < 100:
        print("\n‚ùå TH·∫§T B·∫†I: Kh√¥ng ƒë·ªß d·ªØ li·ªáu sau khi x·ª≠ l√Ω")
        return False
    
    # =========================================================================
    # 3. SPLIT DATA
    # =========================================================================
    print("\n[B∆Ø·ªöC 3/5] ‚úÇÔ∏è  ƒêang chia d·ªØ li·ªáu train/test...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=0.2, 
        random_state=42, 
        shuffle=False  # Gi·ªØ th·ª© t·ª± th·ªùi gian
    )
    
    print(f"   ‚úÖ Train size: {len(X_train):,} samples")
    print(f"   ‚úÖ Test size:  {len(X_test):,} samples")
    print(f"   ‚úÖ Features:   {len(feature_cols)}")
    
    # =========================================================================
    # 4. TRAIN MODEL
    # =========================================================================
    print("\n[B∆Ø·ªöC 4/5] ü§ñ ƒêang hu·∫•n luy·ªán m√¥ h√¨nh XGBoost...")
    print("   ‚è≥ Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t...")
    
    model = MultiOutputRegressor(
        XGBRegressor(
            n_estimators=1000,          # S·ªë c√¢y quy·∫øt ƒë·ªãnh
            learning_rate=0.05,         # T·ªëc ƒë·ªô h·ªçc
            max_depth=10,               # ƒê·ªô s√¢u t·ªëi ƒëa c·ªßa c√¢y
            subsample=0.8,              # T·ª∑ l·ªá m·∫´u con
            colsample_bytree=0.8,       # T·ª∑ l·ªá feature cho m·ªói c√¢y
            random_state=42,
            n_jobs=-1,                  # S·ª≠ d·ª•ng t·∫•t c·∫£ CPU
            tree_method='hist',         # Faster training
            min_child_weight=3,         # Regularization
            gamma=0.1,                  # Regularization
            reg_alpha=0.1,              # L1 regularization
            reg_lambda=1.0,             # L2 regularization
            verbosity=0,                 # T·∫Øt log c·ªßa XGBoost
            enable_categorical=True
        )
    )
    
    try:
        model.fit(X_train, y_train)
        print("   ‚úÖ Ho√†n th√†nh hu·∫•n luy·ªán!")
    except Exception as e:
        print(f"\n‚ùå TH·∫§T B·∫†I: L·ªói khi train model: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # =========================================================================
    # 5. EVALUATE MODEL
    # =========================================================================
    print("\n[B∆Ø·ªöC 5/5] üìä ƒêang ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test...")
    
    try:
        pred = model.predict(X_test)
    except Exception as e:
        print(f"\n‚ùå TH·∫§T B·∫†I: L·ªói khi predict: {e}")
        return False
    
    target_names = [
        'üå°Ô∏è  Nhi·ªát ƒë·ªô (¬∞C)', 
        'üíß ƒê·ªô ·∫©m (%)', 
        'üåßÔ∏è  L∆∞·ª£ng m∆∞a (mm)', 
        'üå¨Ô∏è  T·ªëc ƒë·ªô gi√≥ (km/h)', 
        'üîµ √Åp su·∫•t (hPa)', 
        '‚òÅÔ∏è  ƒê·ªô ph·ªß m√¢y (%)'
    ]
    
    print("\n" + "="*80)
    print("üìà K·∫æT QU·∫¢ ƒê√ÅNH GI√Å M√î H√åNH (D·ª± ƒëo√°n 1 gi·ªù t·ªõi)")
    print("="*80)
    
    overall_score = 0
    scores = []
    
    for i, name in enumerate(target_names):
        mae = mean_absolute_error(y_test.iloc[:, i], pred[:, i])
        rmse = np.sqrt(mean_squared_error(y_test.iloc[:, i], pred[:, i]))
        r2 = r2_score(y_test.iloc[:, i], pred[:, i])
        
        print(f"\n{name}:")
        print(f"   ‚Ä¢ MAE:  {mae:.3f}")
        print(f"   ‚Ä¢ RMSE: {rmse:.3f}")
        print(f"   ‚Ä¢ R¬≤:   {r2:.3f}", end="")
        
        # ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng
        if r2 > 0.90:
            print(" ‚úÖ XU·∫§T S·∫ÆC")
            scores.append(3)
        elif r2 > 0.80:
            print(" ‚úÖ T·ªêT")
            scores.append(2)
        elif r2 > 0.70:
            print(" ‚ö†Ô∏è  CH·∫§P NH·∫¨N ƒê∆Ø·ª¢C")
            scores.append(1)
        else:
            print(" ‚ùå C·∫¶N C·∫¢I THI·ªÜN")
            scores.append(0)
    
    overall_score = np.mean(scores)
    
    # =========================================================================
    # 6. SAVE MODEL
    # =========================================================================
    print("\n" + "="*80)
    print("üíæ ƒêANG L∆ØU M√î H√åNH...")
    
    try:
        joblib.dump(model, save_path)
        joblib.dump(feature_cols, os.path.join(MODEL_DIR, 'feature_cols.pkl'))
        print(f"‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh t·∫°i: {save_path}")
        print(f"‚úÖ ƒê√£ l∆∞u feature columns t·∫°i: {os.path.join(MODEL_DIR, 'feature_cols.pkl')}")
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u model: {e}")
        return False
    
    # =========================================================================
    # 7. FINAL SUMMARY
    # =========================================================================
    print("\n" + "="*80)
    print("üéØ T·ªîNG K·∫æT")
    print("="*80)
    print(f"ƒêi·ªÉm trung b√¨nh: {overall_score:.2f}/3.0")
    
    if overall_score >= 2.5:
        print("\nüåüüåüüåü M√î H√åNH XU·∫§T S·∫ÆC - ƒê·ªò TIN C·∫¨Y R·∫§T CAO!")
        print("‚úÖ C√≥ th·ªÉ s·ª≠ d·ª•ng cho d·ª± ƒëo√°n th·ªùi ti·∫øt th·ª±c t·∫ø")
    elif overall_score >= 1.5:
        print("\n‚≠ê‚≠ê M√î H√åNH T·ªêT - ƒê·ªò TIN C·∫¨Y CH·∫§P NH·∫¨N ƒê∆Ø·ª¢C")
        print("‚úÖ Ph√π h·ª£p cho ·ª©ng d·ª•ng th·ª±c t·∫ø")
    elif overall_score >= 1.0:
        print("\n‚ö†Ô∏è  M√î H√åNH TRUNG B√åNH - N√äN C·∫¢I THI·ªÜN")
        print("üí° Thu th·∫≠p th√™m d·ªØ li·ªáu v√† ƒëi·ªÅu ch·ªânh hyperparameters")
    else:
        print("\n‚ùå M√î H√åNH Y·∫æU - C·∫¶N C·∫¢I THI·ªÜN")
        print("üí° C·∫ßn thu th·∫≠p nhi·ªÅu d·ªØ li·ªáu h∆°n (√≠t nh·∫•t 10,000 samples)")
    
    print("\n" + "="*80)
    print("‚úÖ HO√ÄN T·∫§T HU·∫§N LUY·ªÜN!")
    print("="*80)
    
    print("\nüìù B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh b·∫±ng c√°ch:")
    print("   from machine_learning.predictor import predict_storm")
    print("   result = predict_storm(province_id=1)")
    print("\nüí° ƒê·ªÉ test ƒë·ªô ch√≠nh x√°c:")
    print("   python test_ml_accuracy.py")
    print("   python evaluate_dashboard.py")
    print()
    
    return True

def retrain_for_province(province_id):
    """
    Hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh cho m·ªôt t·ªânh c·ª• th·ªÉ
    
    Args:
        province_id: ID c·ªßa t·ªânh c·∫ßn train
    
    Returns:
        bool: True n·∫øu th√†nh c√¥ng
    """
    model_path = os.path.join(MODEL_DIR, f'weather_xgboost_province_{province_id}.pkl')
    return train(province_id=province_id, save_path=model_path)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Train m√¥ h√¨nh d·ª± ƒëo√°n th·ªùi ti·∫øt')
    parser.add_argument('--province_id', type=int, help='ID t·ªânh c·∫ßn train (b·ªè qua ƒë·ªÉ train t·∫•t c·∫£)')
    
    args = parser.parse_args()
    
    # Hu·∫•n luy·ªán m√¥ h√¨nh
    success = train(province_id=args.province_id)
    
    if not success:
        print("\n‚ùå Training th·∫•t b·∫°i!")
        sys.exit(1)
    
    sys.exit(0)